# 2-Stage CHAMP Training: M2PEncoder + StableAnimator

ì´ ë¬¸ì„œëŠ” **M2PEncoder + StableAnimator**ë¥¼ í†µí•©í•˜ì—¬ 2-stage í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. CHAMP ë…¼ë¬¸ì˜ í•™ìŠµ ì ˆì°¨ë¥¼ ì°¸ê³ í•˜ì—¬ ì¬êµ¬ì„±ëœ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ì „ëµ ê°œìš”

### **í•µì‹¬ ì•„ì´ë””ì–´**
1. **Stage A**: M2PEncoder ë‹¨ë… ì‚¬ì „í•™ìŠµìœ¼ë¡œ ìŒì•…â†’í¬ì¦ˆ ë³€í™˜ ì•ˆì •í™”
2. **Stage B**: ì´ë¯¸ì§€ ì¤‘ì‹¬ í•™ìŠµìœ¼ë¡œ ID/í¬ì¦ˆ ì¡°ê±´ ìˆ˜ìš© ëŠ¥ë ¥ í–¥ìƒ
3. **Stage C**: ëª¨ì…˜/ì‹œê°„ ì¼ê´€ì„± ì¤‘ì‹¬ìœ¼ë¡œ Temporal ëª¨ë“ˆ ìµœì í™”

### **í•™ìŠµ íë¦„**
```
Stage A (M2P Pretrain) â†’ Stage B (Image Training) â†’ Stage C (Motion Training)
     â†“                        â†“                        â†“
ìŒì•…â†’í¬ì¦ˆ ì•ˆì •í™”        Guidance/UNet ì ì‘        Temporal ëª¨ë“ˆ ìµœì í™”
```

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
dance_with_music/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ m2p_encoder.py              # Music-to-Pose Encoder
â”‚   â”œâ”€â”€ integrated_champ_model.py    # í†µí•© CHAMP ëª¨ë¸
â”‚   â”œâ”€â”€ guidance_encoder.py          # Guidance Encoder
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train_2stage.py                  # 2-stage í›ˆë ¨ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ command_train_2stage.sh          # Linux/Mac ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ command_train_2stage.bat         # Windows ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README_2STAGE_TRAINING.md        # ì´ ë¬¸ì„œ
```

## ğŸš€ í•™ìŠµ ì‹¤í–‰ ë°©ë²•

### **1. ìë™ ë‹¨ê³„ ì§„í–‰ (ê¶Œì¥)**
```bash
# Linux/Mac
chmod +x command_train_2stage.sh
./command_train_2stage.sh

# Windows
command_train_2stage.bat
```

### **2. ìˆ˜ë™ ë‹¨ê³„ë³„ ì‹¤í–‰**
```bash
# Stage A: M2P Pretrain
python train_2stage.py --training_stage A --stage_a_steps 5000

# Stage B: Image Training
python train_2stage.py --training_stage B --stage_b_steps 10000

# Stage C: Motion Training
python train_2stage.py --training_stage C --stage_c_steps 15000
```

## ğŸ“Š ë‹¨ê³„ë³„ ìƒì„¸ í•™ìŠµ ì „ëµ

### **Stage A: M2PEncoder ë‹¨ë… ì‚¬ì „í•™ìŠµ**

#### **ëª©í‘œ**
- ìŒì•…â†’í¬ì¦ˆ íˆíŠ¸ë§µ/ì¢Œí‘œ ì•ˆì •ì  ì˜ˆì¸¡
- ArcFace í˜¸í™˜ ì–¼êµ´ ì„ë² ë”© ìƒì„±

#### **ì…ë ¥/ì¶œë ¥**
- **ì…ë ¥**: ì˜¤ë””ì˜¤ ìœˆë„ìš° `A`, ì°¸ì¡° ì–¼êµ´ ì´ë¯¸ì§€ `I_ref`
- **GT**: DWpose íˆíŠ¸ë§µ/ì¢Œí‘œ, ArcFace ì„ë² ë”© (512-d, L2-norm)

#### **ì†ì‹¤ í•¨ìˆ˜**
```python
L_total = L_heat + L_face

# Pose íˆíŠ¸ë§µ ì†ì‹¤
L_heat = CrossEntropy(pose_logits, target_heatmap) + 
          L1(soft_argmax(pred), target_coordinates)

# Face ì„ë² ë”© ì†ì‹¤
L_face = 1 - cos(E_pred, E_ArcFace)  # ë˜ëŠ” L2
```

#### **Freeze ì „ëµ**
- **Freeze**: ArcFace (ì™„ì „ ê³ ì •), VAE, CLIP
- **Train**: M2PEncoderë§Œ í•™ìŠµ

#### **í•™ìŠµë¥ **: `1e-4`

---

### **Stage B: ê²°í•© ì´ë¯¸ì§€ ë‹¨ê³„**

#### **ëª©í‘œ**
- StableAnimator UNetì´ **ID/í¬ì¦ˆ ì¡°ê±´ì„ ìˆ˜ìš©**í•˜ë„ë¡ ì ì‘
- Guidance ê²½ë¡œì™€ ReferenceNet ìµœì í™”

#### **ì…ë ¥/ì¶œë ¥**
- **ì…ë ¥**: `face_emb`(M2P), `heatmap`(M2P), ì°¸ì¡° ì´ë¯¸ì§€
- **ì¶œë ¥**: ì¡°ê±´ë¶€ ë¹„ë””ì˜¤ ìƒì„±

#### **ì†ì‹¤ í•¨ìˆ˜**
```python
L_total = L_diff + L_id + L_heat

# í™•ì‚° ì†ì‹¤
L_diff = MSE(model_pred, target_latents)

# Identity ì†ì‹¤ (ì–¼êµ´ ë§ˆìŠ¤í¬ ê°€ì¤‘)
L_id = ArcFace/CLIP-ID loss (face_mask_weighted)

# Pose ì†ì‹¤
L_heat = CrossEntropy(pose_pred, target_pose)
```

#### **Freeze ì „ëµ**
- **Freeze**: VAE ì¸ì½”ë”/ë””ì½”ë”, CLIP ì´ë¯¸ì§€ ì¸ì½”ë”
- **Train**: Guidance ê²½ë¡œ, Denoising UNet, ReferenceNet
- **M2PEncoder**: ì²˜ìŒì—ëŠ” **ê³ ì •** â†’ ìˆ˜ë ´ í›„ **ë‚®ì€ LR**ë¡œ í’€ê¸°

#### **í•™ìŠµë¥ **: 
- Guidance/UNet: `1e-4`
- M2PEncoder: `1e-5` (í›„ë°˜ë¶€)

---

### **Stage C: ëª¨ì…˜ ë‹¨ê³„**

#### **ëª©í‘œ**
- ì‹œê°„ ì¼ê´€ì„±/ë¦¬ë“¬ ê°•í™”
- Temporal Attention/ëª¨ë“ˆ ìµœì í™”

#### **ì…ë ¥/ì¶œë ¥**
- **ì…ë ¥**: T=24~150 í”„ë ˆì„ ì‹œí€€ìŠ¤, `face_emb`, `heatmap`(M2P)
- **ì¶œë ¥**: ì‹œê°„ ì¼ê´€ì„± ìˆëŠ” ë¹„ë””ì˜¤

#### **ì†ì‹¤ í•¨ìˆ˜**
```python
L_total = L_diff + L_temp + L_id

# í™•ì‚° ì†ì‹¤
L_diff = MSE(model_pred, target_latents)

# Temporal ì¼ê´€ì„± ì†ì‹¤
L_temp = Flow/ì°¨ë¶„ ì •í•© ì†ì‹¤

# Identity ì†ì‹¤ (ì–¼êµ´ ê°€ì¤‘â†‘)
L_id = Face-weighted identity loss
```

#### **Freeze ì „ëµ**
- **Freeze**: Stage Bì—ì„œ í•™ìŠµí•œ Guidance/UNet/ReferenceNet
- **Train**: Temporal ëª¨ë“ˆë§Œ í•™ìŠµ
- **M2PEncoder**: ê¸°ë³¸ **ê³ ì •**, í•„ìš”ì‹œ **ì†ŒLR**ë¡œ ë™ê²° í•´ì œ

#### **í•™ìŠµë¥ **: `5e-5`

## ğŸ”§ ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ì •

### **í•™ìŠµë¥  ì„¤ì •**
```bash
--stage_a_lr 1e-4        # Stage A: M2P pretrain
--stage_b_lr 1e-4        # Stage B: Image training
--stage_c_lr 5e-5        # Stage C: Motion training
--m2p_lr_stage_b 1e-5   # M2P LR in Stage B
```

### **ì†ì‹¤ ê°€ì¤‘ì¹˜**
```bash
--diffusion_loss_weight 1.0    # í™•ì‚° ì†ì‹¤
--pose_loss_weight 0.5         # í¬ì¦ˆ ì†ì‹¤
--face_loss_weight 0.1         # ì–¼êµ´ ì†ì‹¤
--id_loss_weight 0.1           # Identity ì†ì‹¤
--temporal_loss_weight 0.1     # Temporal ì†ì‹¤
```

### **ë‹¨ê³„ë³„ ìŠ¤í… ìˆ˜**
```bash
--stage_a_steps 5000    # M2P pretrain
--stage_b_steps 10000   # Image training
--stage_c_steps 15000   # Motion training
```

## ğŸ›ï¸ Freeze ë§¤íŠ¸ë¦­ìŠ¤ (ëª¨ë“ˆë³„)

| ëª¨ë“ˆ | Stage A | Stage B | Stage C |
|------|---------|---------|---------|
| **M2PEncoder** | ğŸŸ¢ Train | ğŸŸ¡ Low LR | ğŸ”´ Freeze |
| **ArcFace** | ğŸ”´ Freeze | ğŸ”´ Freeze | ğŸ”´ Freeze |
| **Pose Adapter/Control** | â€” | ğŸŸ¢ Train | ğŸ”´ Freeze |
| **UNet(Spatial)** | â€” | ğŸŸ¢ Train | ğŸ”´ Freeze |
| **Temporal ëª¨ë“ˆ** | â€” | â€” | ğŸŸ¢ Train |
| **ReferenceNet** | â€” | ğŸŸ¢ Train | ğŸ”´ Freeze |
| **VAE Enc/Dec** | â€” | ğŸ”´ Freeze | ğŸ”´ Freeze |
| **CLIP Image Encoder** | â€” | ğŸ”´ Freeze | ğŸ”´ Freeze |

**ğŸŸ¢ Train**: í™œì„± í•™ìŠµ, **ğŸŸ¡ Low LR**: ë‚®ì€ í•™ìŠµë¥ , **ğŸ”´ Freeze**: ë™ê²°

## ğŸ’¡ ì‹¤ì „ ìš´ìš© íŒ

### **1. Scheduled Sampling**
```python
# Stage Bâ†’C ì „í™˜ ì‹œ
if stage == "B" and step > stage_b_steps * 0.8:
    # ì ì§„ì ìœ¼ë¡œ M2P ì˜ˆì¸¡ìœ¼ë¡œ ì¹˜í™˜
    guidance_ratio = min(1.0, (step - stage_b_steps * 0.8) / (stage_b_steps * 0.2))
    guidance = guidance_ratio * m2p_pred + (1 - guidance_ratio) * gt_pose
```

### **2. Condition Scale ìŠ¤ì¼€ì¤„**
```python
# í•™ìŠµ ì´ˆë°˜ í¬ì¦ˆ ê°•ì œë ¥ ì ì§„ì  ìƒìŠ¹
guidance_scale = min(1.5, 0.5 + step / (total_steps * 0.3))
```

### **3. Detach í™œìš©**
```python
# ì´ˆê¸° E2E ê²°í•© ì‹œ ì—­ì „íŒŒ ì°¨ë‹¨
with torch.no_grad():
    m2p_output = m2p_encoder(music_features)
    guidance_condition = m2p_output['pose_heatmap'].detach()
```

### **4. ID ì•ˆì •í™”**
```python
# ì–¼êµ´ ë§ˆìŠ¤í¬ ê°€ì¤‘
face_mask = create_face_mask(reference_image)
id_loss = id_loss * face_mask * 3.0  # ì–¼êµ´ ì¤‘ì‹¬ ê°€ì¤‘ì¹˜
```

### **5. Temporal ë°°ì¹˜ ì „ëµ**
```python
# T=24ë¡œ ë¹ ë¥¸ ìˆ˜ë ´ â†’ ì ì°¨ Tâ†‘
sequence_length = min(150, 24 + epoch // 10)
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦

### **TensorBoard ë©”íŠ¸ë¦­**
```bash
tensorboard --logdir logs
```

### **ì£¼ìš” ì¶”ì  ì§€í‘œ**
- **Stage A**: `pose_loss`, `face_loss`
- **Stage B**: `diffusion_loss`, `pose_loss`, `id_loss`
- **Stage C**: `diffusion_loss`, `temporal_loss`, `id_loss`

### **ê²€ì¦ ì „ëµ**
- **Stage A**: Pose ì •í™•ë„, Face ì„ë² ë”© í’ˆì§ˆ
- **Stage B**: ì´ë¯¸ì§€ í’ˆì§ˆ, ID ìœ ì§€, í¬ì¦ˆ ì •í™•ë„
- **Stage C**: ì‹œê°„ ì¼ê´€ì„±, ëª¨ì…˜ ìì—°ìŠ¤ëŸ¬ì›€

## ğŸš¨ ë¬¸ì œ í•´ê²°

### **ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
--train_batch_size 1
--gradient_accumulation_steps 8
--mixed_precision fp16
```

### **í›ˆë ¨ ë¶ˆì•ˆì •**
```bash
--learning_rate 5e-5        # LR ê°ì†Œ
--lr_warmup_steps 1000      # Warmup ì¦ê°€
--gradient_clip_norm 0.5    # Gradient clipping
```

### **Guidance íš¨ê³¼ ë¶€ì¡±**
```bash
--guidance_scale 1.5        # Guidance ê°•ë„ ì¦ê°€
--pose_loss_weight 1.0      # Pose loss ê°€ì¤‘ì¹˜ ì¦ê°€
```

### **Temporal í•™ìŠµ ì–´ë ¤ì›€**
```bash
--temporal_loss_weight 0.2  # Temporal loss ê°€ì¤‘ì¹˜ ì¦ê°€
--sequence_length 48         # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### **ë°ì´í„° ë¡œë”©**
```bash
--num_workers 8              # Worker ìˆ˜ ì¦ê°€
--pin_memory true            # GPU ë©”ëª¨ë¦¬ í™œìš©
--prefetch_factor 2          # í”„ë¦¬í˜ì¹˜ ìµœì í™”
```

### **ëª¨ë¸ ìµœì í™”**
```bash
--mixed_precision fp16       # Mixed precision
--gradient_checkpointing     # ë©”ëª¨ë¦¬ ì ˆì•½
--use_xformers               # XFormers attention
```

## ğŸ”„ ë‹¨ê³„ë³„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### **ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°**
```
champ_2stage_output/
â”œâ”€â”€ stage_A/
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ checkpoint-2000/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ stage_B/
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ checkpoint-2000/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ stage_C/
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ checkpoint-2000/
â”‚   â””â”€â”€ ...
â””â”€â”€ final/
```

### **ì²´í¬í¬ì¸íŠ¸ ë¡œë”©**
```python
# íŠ¹ì • ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
accelerator.load_state("champ_2stage_output/stage_B/checkpoint-5000")

# ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
model.set_training_stage("C")
```

## ğŸ¯ ìµœì¢… ì •ë¦¬

### **í•µì‹¬ í•™ìŠµ ìˆœì„œ**
1. **M2PEncoder ì•ˆì •í™”** (Stage A)
2. **Guidance/UNet ì ì‘** (Stage B, VAE/CLIP ë™ê²°)
3. **Temporal ëª¨ë“ˆ ìµœì í™”** (Stage C, ë‚˜ë¨¸ì§€ ë™ê²°)
4. **í•„ìš”ì‹œ ì €LR í•©ë¯¸ì„¸ì¡°ì •**

### **ì„±ê³µ ìš”ì¸**
- **ë‹¨ê³„ë³„ ëª…í™•í•œ ëª©í‘œ** ì„¤ì •
- **ì ì ˆí•œ Freeze ì „ëµ** ì ìš©
- **ë‹¨ê³„ë³„ í•™ìŠµë¥ ** ì¡°ì •
- **ì†ì‹¤ ê°€ì¤‘ì¹˜** ê· í˜•
- **ì²´ê³„ì ì¸ ê²€ì¦** ë° ëª¨ë‹ˆí„°ë§

ì´ 2-stage í•™ìŠµ ì‹œìŠ¤í…œì„ í†µí•´ **M2PEncoder + StableAnimator**ê°€ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©ë˜ì–´, ìŒì•…ì— ë§ì¶˜ ìì—°ìŠ¤ëŸ¬ìš´ ì¶¤ ë™ì‘ì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
